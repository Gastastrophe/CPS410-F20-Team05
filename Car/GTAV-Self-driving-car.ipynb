{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grand Theft Auto V Driving learning with Deep Learning (CNN and YOLO)\n",
    "---\n",
    "Self driving car in Grand Theft Auto V with Deep Learning and Object Detection. Adapted from\n",
    "https://github.com/eritzyg/GTAV-Self-driving-car\n",
    "by Iker Garcia and Eritz Yerga.\n",
    "\n",
    "\n",
    "\n",
    "### Authors: Evan Miller, Allan Bourke, Joshua Gutowski, and Ian Kraft\n",
    "\n",
    "See [this README](https://github.com/eritzyg/GTAV-Self-driving-car/blob/master/Notebook/README.md) for info about the notebook and required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Index:\n",
    "\n",
    "0. <a href=\"#0.-Object-Detection\">Object Detection</a>\n",
    "1. <a href=\"#1.-Generate-dataset\">Generate dataset</a>\n",
    "    * <a href=\"#Frame-capture-functions\">Frame capture functions</a>\n",
    "    * <a href=\"#Image-preprocessing-functions\">Image preprocess functions</a>\n",
    "    * <a href=\"#Game-control-and-input-reading-functions\">Game control and input reading functions</a>\n",
    "    * <b><a href=\"#Generate-dataset\">Generate dataset</a></b>\n",
    "2. <a href=\"#2.-Dataset-processing-utilities\">Dataset processing utilities</a>\n",
    "3. <a href=\"#3.-Define-the-model\">Define our model</a>\n",
    "4. <a href=\"#4.-Train\">Train</a>\n",
    "5. <a href=\"#5.-Run-our-model-in-the-game\">Run our model in the game</a>\n",
    "\n",
    "<a href=\"\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "First we import the libraries we are going to need to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:%APPDATA%\\Python\\Python38\\Scripts')\n",
    "\n",
    "\n",
    "!pip install opencv-python\n",
    "!pip install tensorflow --ignore-installed --user\n",
    "\n",
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emill\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\emill\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\Users\\emill\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import ImageGrab\n",
    "import cv2 \n",
    "import time\n",
    "from sys import stdout\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from grabber import Grabber\n",
    "import threading\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "import glob\n",
    "import tensorflow.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import TimeDistributed, LSTM, Flatten, Dense, InputLayer, MaxPooling2D, Dropout, Activation, Embedding, GRU, ConvLSTM2D, concatenate\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import initializers\n",
    "import h5py\n",
    "import log\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets an array of filepaths to images from a folder\n",
    "\n",
    "def loadImages(path):\n",
    "    # Put files into lists and return them as one list of size 4\n",
    "    image_files = sorted([os.path.join(path, 'train', file)\n",
    "         for file in os.listdir(path + \"/train\") if      file.endswith('.png')])\n",
    " \n",
    "    return image_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Object Detection\n",
    "By Evan Miller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section utilizes the Yolov3 object detection model with pretrained weights for driving created by Lavanya Shukla on Kaggle. https://www.kaggle.com/lavanyashukla01/yolov3-lyft-dataset/?select=model.h5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the model from the file\n",
    "obj_model = load_model('yolo.h5', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By Lavanya Shukla on Kaggle.\n",
    "\n",
    "#Custom class for functionality needed to extract bounding boxes from the YOLO model\n",
    "\n",
    "class BoundBox:\n",
    "    def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n",
    "        self.xmin = xmin\n",
    "        self.ymin = ymin\n",
    "        self.xmax = xmax\n",
    "        self.ymax = ymax\n",
    "        self.objness = objness\n",
    "        self.classes = classes\n",
    "        self.label = -1\n",
    "        self.score = -1\n",
    "\n",
    "    def get_label(self):\n",
    "        if self.label == -1:\n",
    "            self.label = np.argmax(self.classes)\n",
    "\n",
    "        return self.label\n",
    "\n",
    "    def get_score(self):\n",
    "        if self.score == -1:\n",
    "            self.score = self.classes[self.get_label()]\n",
    " \n",
    "        return self.score\n",
    " \n",
    "def _sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "def _softmax(x, axis=-1):\n",
    "    x = x - np.amax(x, axis, keepdims=True)\n",
    "    e_x = np.exp(x)\n",
    "    \n",
    "    return e_x / e_x.sum(axis, keepdims=True)\n",
    " \n",
    "def decode_netout(netout, anchors, obj_thresh, net_h, net_w):\n",
    "    grid_h, grid_w = netout.shape[:2]\n",
    "    nb_box = 3\n",
    "    netout = netout.reshape((grid_h, grid_w, nb_box, -1))\n",
    "    nb_class = netout.shape[-1] - 5\n",
    "\n",
    "    boxes = []\n",
    "\n",
    "    netout[..., :2]  = _sigmoid(netout[..., :2])\n",
    "    netout[..., 4]   = _sigmoid(netout[..., 4])\n",
    "    netout[..., 5:]  = netout[..., 4][..., np.newaxis] * _softmax(netout[..., 5:])\n",
    "    netout[..., 5:] *= netout[..., 5:] > obj_thresh\n",
    "\n",
    "    for i in range(grid_h*grid_w):\n",
    "        row = i // grid_w\n",
    "        col = i % grid_w\n",
    "        \n",
    "        for b in range(nb_box):\n",
    "            # 4th element is objectness score\n",
    "            objectness = netout[row, col, b, 4]\n",
    "            \n",
    "            if(objectness <= obj_thresh): continue\n",
    "            \n",
    "            # first 4 elements are x, y, w, and h\n",
    "            x, y, w, h = netout[row,col,b,:4]\n",
    "\n",
    "            x = (col + x) / grid_w # center position, unit: image width\n",
    "            y = (row + y) / grid_h # center position, unit: image height\n",
    "            w = anchors[2 * b + 0] * np.exp(w) / net_w # unit: image width\n",
    "            h = anchors[2 * b + 1] * np.exp(h) / net_h # unit: image height  \n",
    "            \n",
    "            # last elements are class probabilities\n",
    "            classes = netout[row,col,b,5:]\n",
    "            \n",
    "            box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n",
    "\n",
    "            boxes.append(box)\n",
    "\n",
    "    return boxes\n",
    "\n",
    "#TODO: Minimize implementation for images that don't need resizing\n",
    "def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n",
    "    if (float(net_w)/image_w) < (float(net_h)/image_h):\n",
    "        new_w = net_w\n",
    "        new_h = (image_h*net_w)/image_w\n",
    "    else:\n",
    "        new_h = net_w\n",
    "        new_w = (image_w*net_h)/image_h\n",
    "        \n",
    "    for i in range(len(boxes)):\n",
    "        x_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w\n",
    "        y_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h\n",
    "        \n",
    "        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n",
    "        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n",
    "        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n",
    "        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)\n",
    "        \n",
    "\n",
    "def _interval_overlap(interval_a, interval_b):\n",
    "    x1, x2 = interval_a\n",
    "    x3, x4 = interval_b\n",
    "    if x3 < x1:\n",
    "        if x4 < x1:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x1\n",
    "    else:\n",
    "        if x2 < x3:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x3\n",
    "\n",
    "def bbox_iou(box1, box2):\n",
    "    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
    "    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n",
    "    intersect = intersect_w * intersect_h\n",
    "    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
    "    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
    "    union = w1*h1 + w2*h2 - intersect\n",
    "    return float(intersect) / union\n",
    "\n",
    "def do_nms(boxes, nms_thresh):\n",
    "    if len(boxes) > 0:\n",
    "        nb_class = len(boxes[0].classes)\n",
    "    else:\n",
    "        return\n",
    "        \n",
    "    for c in range(nb_class):\n",
    "        sorted_indices = np.argsort([-box.classes[c] for box in boxes])\n",
    "\n",
    "        for i in range(len(sorted_indices)):\n",
    "            index_i = sorted_indices[i]\n",
    "\n",
    "            if boxes[index_i].classes[c] == 0: continue\n",
    "\n",
    "            for j in range(i+1, len(sorted_indices)):\n",
    "                index_j = sorted_indices[j]\n",
    "\n",
    "                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n",
    "                    boxes[index_j].classes[c] = 0\n",
    "\n",
    "# load and prepare an image\n",
    "def load_image_pixels(filename, shape):\n",
    "    # load the image to get its shape\n",
    "    image = load_img(filename)\n",
    "    width, height = image.size\n",
    "    # load the image with the required size\n",
    "    image = load_img(filename, target_size=shape)\n",
    "    # convert to numpy array\n",
    "    image = img_to_array(image)\n",
    "    # scale pixel values to [0, 1]\n",
    "    image = image.astype('float32')\n",
    "    image /= 255.0\n",
    "    # add a dimension so that we have one sample\n",
    "    image = expand_dims(image, 0)\n",
    "    return image, width, height\n",
    "\n",
    "# get all of the results above a threshold\n",
    "def get_boxes(boxes, labels, thresh):\n",
    "    v_boxes, v_labels, v_scores = list(), list(), list()\n",
    "    # enumerate all boxes\n",
    "    for box in boxes:\n",
    "        # enumerate all possible labels\n",
    "        for i in range(len(labels)):\n",
    "            # check if the threshold for this label is high enough\n",
    "            if box.classes[i] > thresh:\n",
    "                v_boxes.append(box)\n",
    "                v_labels.append(labels[i])\n",
    "                v_scores.append(box.classes[i]*100)\n",
    "                # don't break, many labels may trigger for one box\n",
    "    return v_boxes, v_labels, v_scores\n",
    "\n",
    "# draw all results\n",
    "def draw_boxes(filename, v_boxes, v_labels, v_scores):\n",
    "    # load the image\n",
    "    data = pyplot.imread(filename)\n",
    "    # plot the image\n",
    "    pyplot.imshow(data)\n",
    "    # get the context for drawing boxes\n",
    "    ax = pyplot.gca()\n",
    "    # plot each box\n",
    "    for i in range(len(v_boxes)):\n",
    "        box = v_boxes[i]\n",
    "        # get coordinates\n",
    "        y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n",
    "        # calculate width and height of the box\n",
    "        width, height = x2 - x1, y2 - y1\n",
    "        # create the shape\n",
    "        rect = Rectangle((x1, y1), width, height, fill=False, color='white')\n",
    "        # draw the box\n",
    "        ax.add_patch(rect)\n",
    "        # draw text and score in top left corner\n",
    "        label = \"%s (%.3f)\" % (v_labels[i], v_scores[i])\n",
    "        pyplot.text(x1, y1, label, color='white')\n",
    "    # show the plot\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a normalized rgb image file and returns a tuple of two image arrays. The first has the objects and the second has everything else.\n",
    "#Adapted from Lavanya Shukla on Kaggle. Written by Evan Miller.\n",
    "\n",
    "# VERSION 1: KEPT FOR DEBUGGING PURPOSES. DELETE WHEN FINISHED!\n",
    "\n",
    "def get_sub_images(imagepath):\n",
    "    #get image size and model paramaters\n",
    "    WIDTH, HEIGHT = 416,416\n",
    "    anchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]]\n",
    "    class_threshold = 0.3\n",
    "    \n",
    "    #load original image\n",
    "    image = load_img(filename)\n",
    "    width, height = image.size\n",
    "    IMAGE = img_to_array(image)\n",
    "    \n",
    "    # load the image with the required size\n",
    "    image = load_img(filename, target_size=(WIDTH,HEIGHT))\n",
    "    \n",
    "    # convert to numpy array\n",
    "    image = img_to_array(image)\n",
    "    \n",
    "    #normalize image and add a dimension\n",
    "    image = image.astype('float32')\n",
    "    image /= 255.0\n",
    "    image = expand_dims(image, 0)\n",
    "    \n",
    "    yhat = obj_model.predict(image)\n",
    "    \n",
    "    # Create boxes\n",
    "    boxes = list()\n",
    "    for i in range(len(yhat)):\n",
    "        # decode the output of the network\n",
    "        boxes += decode_netout(yhat[i][0], anchors[i], class_threshold, HEIGHT, WIDTH)\n",
    "    \n",
    "    # correct the sizes of the bounding boxes for the shape of the image\n",
    "    correct_yolo_boxes(boxes, HEIGHT, WIDTH, HEIGHT, WIDTH)\n",
    "    \n",
    "    # suppress non-maximal boxes\n",
    "    do_nms(boxes, 0.5)\n",
    "    \n",
    "    # get the details of the detected objects\n",
    "    v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)\n",
    "    \n",
    "    objects = np.zeros_like(IMAGE)\n",
    "    background = IMAGE[:]\n",
    "    \n",
    "    for i in range(len(v_boxes)):\n",
    "        box = v_boxes[i]\n",
    "        # get coordinates\n",
    "        y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n",
    "        # Loop over area of each box\n",
    "        for i in range(x1,x2):\n",
    "            for j in range(y1,y2):\n",
    "                # Data is channels last, so we copy and erase whole channels here\n",
    "                # objects contains all values within the box\n",
    "                objects[j][i] = IMAGE[j][i][:]\n",
    "                # background gets this coordinate erased and is left with the remainder of the image\n",
    "                background[j][i] = np.zeros_like(IMAGE[j][i])\n",
    "    \n",
    "    return (objects, background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a normalized rgb image and returns a tuple of two image arrays. The first has the objects and the second has everything else.\n",
    "#Adapted from Lavanya Shukla on Kaggle. Written by Evan Miller.\n",
    "\n",
    "def get_sub_images(IMAGE):\n",
    "    #declare labels for the model\n",
    "    labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\"]\n",
    "    \n",
    "    #get image size and model paramaters\n",
    "    WIDTH, HEIGHT = 416,416\n",
    "    anchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]]\n",
    "    class_threshold = 0.3\n",
    "    \n",
    "    #normalize image and add a dimension\n",
    "    IMAGE = IMAGE.astype('float32')\n",
    "    IMAGE /= 255.0\n",
    "    inpt = np.expand_dims(IMAGE, 0)\n",
    "    \n",
    "    yhat = obj_model.predict(inpt)\n",
    "    \n",
    "    # Create boxes\n",
    "    boxes = list()\n",
    "    for i in range(len(yhat)):\n",
    "        # decode the output of the network\n",
    "        boxes += decode_netout(yhat[i][0], anchors[i], class_threshold, HEIGHT, WIDTH)\n",
    "    \n",
    "    # correct the sizes of the bounding boxes for the shape of the image\n",
    "    correct_yolo_boxes(boxes, HEIGHT, WIDTH, HEIGHT, WIDTH)\n",
    "    \n",
    "    # suppress non-maximal boxes\n",
    "    do_nms(boxes, 0.5)\n",
    "    \n",
    "    # get the details of the detected objects\n",
    "    v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)\n",
    "    \n",
    "    objects = np.zeros_like(IMAGE)\n",
    "    background = IMAGE[:]\n",
    "    \n",
    "    for x in range(len(v_boxes)):\n",
    "        box = v_boxes[x]\n",
    "        # get coordinates\n",
    "        y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n",
    "        # Loop over area of each box\n",
    "        for i in range(x1,x2):\n",
    "            for j in range(y1,y2):\n",
    "                # Data is channels last, so we copy and erase whole channels here\n",
    "                # objects contains all values within the box\n",
    "                objects[j][i] = IMAGE[j][i][:]\n",
    "                # background gets this coordinate erased and is left with the remainder of the image\n",
    "                background[j][i] = np.zeros_like(IMAGE[j][i])\n",
    "    \n",
    "    return (objects, background)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generate dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate the dataset for training the model later on, it is important that we set the game on the first person view and take into account certain conditions for the dataset. Check the \"Generation of the dataset\" section in the [documentation](https://github.com/eritzyg/GTAV-Self-driving-car#documentation) for more information on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_control import PressKey, ReleaseKey\n",
    "from getkeys import key_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame capture functions\n",
    "These functions capture the game's frames in 1600x900 Windowed mode.\n",
    "\n",
    "Screen record is the method to get one frame and img_thread is the thread we will later use to constantly capture the game's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "global grb\n",
    "grb = Grabber(bbox=(129,26,1152,1025))\n",
    "def screen_record(method = 'ImageGrab'):\n",
    "    if method == 'ImageGrab':\n",
    "        printscreen =  ImageGrab.grab(bbox=(126,26,1152,1025))\n",
    "        generalIMG = np.array(printscreen)\n",
    "    \n",
    "    elif method == 'grabber':\n",
    "        global grb\n",
    "        printscreen = None\n",
    "        printscreen = grb.grab(printscreen)\n",
    "        generalIMG = np.array(printscreen)\n",
    "    \n",
    "    return generalIMG          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "global front_buffer\n",
    "global back_buffer\n",
    "front_buffer = np.zeros((1024, 1024), dtype=np.int8)\n",
    "back_buffer = np.zeros((1024, 1024), dtype=np.int8)\n",
    "\n",
    "global fps\n",
    "fps = 0\n",
    "\n",
    "def img_thread():\n",
    "    global front_buffer\n",
    "    global back_buffer\n",
    "    global fps\n",
    "    \n",
    "    last_time = time.time()\n",
    "    while True:\n",
    "        front_buffer = screen_record()\n",
    "        # Swap buffers\n",
    "        front_buffer, back_buffer = back_buffer, front_buffer\n",
    "        fps = int(1.0/(time.time()-last_time))\n",
    "        last_time = time.time()\n",
    "        \n",
    "        if 'J' in key_check():\n",
    "            break\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image preprocessing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a function that will apply the preprocessing we want to the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code inspired by https://towardsdatascience.com/image-pre-processing-c1aec0be3edf\n",
    "# Ideas written by Prince Canuma\n",
    "\n",
    "#adds Gaussian blur to an image\n",
    "\n",
    "def blur(res_img):\n",
    "\n",
    "    no_noise = []\n",
    "    for i in range(len(res_img)):\n",
    "        blur = cv2.GaussianBlur(res_img[i], (5, 5), 0)\n",
    "        no_noise.append(blur)\n",
    "\n",
    "    return no_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    proccessed_image = cv2.resize(image,(416,416))\n",
    "    #proccessed_image = blur(proccessed_image)\n",
    "    \n",
    "    return proccessed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game control and input reading functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions will read the inputs and generate a array for later use when generating the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keys_to_output(keys):\n",
    "    '''\n",
    "    Convert keys to a ...multi-hot... array\n",
    "\n",
    "    [A,W,D] boolean values.\n",
    "    '''\n",
    "    output = [0,0,0,0]\n",
    "    \n",
    "    if 'A' in keys:\n",
    "        output[0] = 1\n",
    "    if 'D' in keys:\n",
    "        output[1] = 1\n",
    "    if 'W' in keys:\n",
    "        output[2] = 1\n",
    "    if 'S' in keys:\n",
    "        output[3] = 1\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sequencer thread will capture the sequences of 5 frames with a separation of 1/capturerate ms each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "global seq\n",
    "global num\n",
    "num = 0\n",
    "seq = []\n",
    "\n",
    "global key_out\n",
    "key_out = [0, 0, 0, 0]\n",
    "\n",
    "def image_sequencer_thread():\n",
    "    global back_buffer\n",
    "    global seq\n",
    "    global key_out\n",
    "    global num\n",
    "    \n",
    "    # Frames per second capture rate\n",
    "    capturerate = 10.0\n",
    "    while True:\n",
    "        last_time = time.time()\n",
    "        if len(seq) == 5:\n",
    "            del seq[0]\n",
    "\n",
    "        seq.append(preprocess_image(np.copy(back_buffer)))\n",
    "        num = num + 1\n",
    "        keys = key_check()\n",
    "        if 'J' in keys:\n",
    "            break\n",
    "        key_out = keys_to_output(keys)\n",
    "        waittime = (1.0/capturerate)-(time.time()-last_time)\n",
    "        if waittime>0.0:\n",
    "            time.sleep(waittime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will be useful to check which class corresponds the input we captured to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter_keys(key):\n",
    "        if np.array_equal(key , [0,0,0,0]):\n",
    "            return 0\n",
    "        elif np.array_equal(key , [1,0,0,0]):\n",
    "            return 1\n",
    "        elif np.array_equal(key , [0,1,0,0]):\n",
    "            return 2\n",
    "        elif np.array_equal(key , [0,0,1,0]):\n",
    "            return 3\n",
    "        elif np.array_equal(key , [0,0,0,1]):\n",
    "            return 4\n",
    "        elif np.array_equal(key , [1,0,1,0]):\n",
    "            return 5\n",
    "        elif np.array_equal(key , [1,0,0,1]):\n",
    "            return 6\n",
    "        elif np.array_equal(key , [0,1,1,0]):\n",
    "            return 7\n",
    "        elif np.array_equal(key , [0,1,0,1]):\n",
    "            return 8\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function that the data saving threads will run to save the dataset to compressed files (change the path if needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data,number):\n",
    "    file_name = 'Data\\\\training_data'+str(number)+'.npz'\n",
    "    np.savez_compressed(file_name,data=list([x[:5] for x in data]),labels=list([x[5:] for x in data]))\n",
    "    del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run this function to generate the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    global fps\n",
    "    global front_buffer\n",
    "    global back_buffer\n",
    "    global seq\n",
    "    global key_out\n",
    "    global num\n",
    "    training_data = []\n",
    "    threads = list()\n",
    "    th_img = threading.Thread(target=img_thread)\n",
    "    th_seq = threading.Thread(target=image_sequencer_thread)\n",
    "    threads.append(th_img)\n",
    "    threads.append(th_seq)\n",
    "    th_img.start()\n",
    "    time.sleep(1)\n",
    "    th_seq.start()\n",
    "    l = 0\n",
    "    fn = 0\n",
    "    time.sleep(4)\n",
    "    last_num = 0\n",
    "    \n",
    "    number_of_keys = [0,0,0,0,0,0,0,0,0]\n",
    "    \n",
    "    while True:\n",
    "        img_seq = seq.copy()\n",
    "        output = key_out.copy()\n",
    "        \n",
    "        while len(img_seq) != 5 or last_num==num:\n",
    "            del img_seq, output\n",
    "            img_seq = seq.copy()\n",
    "            output = key_out.copy()\n",
    "        last_num = num\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        stdout.write('Recording at {} FPS \\n'.format(fps))\n",
    "        stdout.write('Images in sequence {} \\n'.format(len(img_seq)))\n",
    "        stdout.write('Training data len {} secuences \\n'.format(l))\n",
    "        stdout.write('Number of archives {}\\n'.format(fn))\n",
    "        stdout.write('Keys pressed: ' + str(output) + ' \\n')\n",
    "        stdout.write('Keys samples in this file: ' + 'none:' + str(number_of_keys[0]) + ' A:' + str(number_of_keys[1])+ ' D:' + str(number_of_keys[2]) + ' W:' + str(number_of_keys[3])+ ' S:' + str(number_of_keys[4]) + ' AW:'  + str(number_of_keys[5]) + ' AS:' + str(number_of_keys[6]) + ' WD:' + str(number_of_keys[7]) + ' SD:' + str(number_of_keys[8]) + ' \\n')\n",
    "        stdout.flush()\n",
    "        \n",
    "        key  = counter_keys(output)\n",
    "        \n",
    "        if key != -1:\n",
    "            larg = nlargest(9,number_of_keys)\n",
    "            prop = (9. - float(larg.index(number_of_keys[key])))/10\n",
    "            if(number_of_keys[key]  > np.mean(number_of_keys) * 1.25):\n",
    "                prop = prop + 0.05\n",
    "            if (np.random.rand() > prop):\n",
    "                number_of_keys[key] += 1\n",
    "                l = l+1\n",
    "                training_data.append([img_seq[0],img_seq[1],img_seq[2],img_seq[3],img_seq[4], output])\n",
    "            \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "        if len(training_data) % 500 == 0 and len(training_data) != 0:\n",
    "            threading.Thread(target=save_data, args=(training_data.copy(), fn,)).start()\n",
    "            fn = fn + 1\n",
    "            del training_data\n",
    "            training_data = []\n",
    "            \n",
    "        if 'J' in key_check():\n",
    "            threading.Thread(target=save_data, args=(training_data.copy(), fn,)).start()\n",
    "            fn = fn + 1\n",
    "            del training_data\n",
    "            training_data = []\n",
    "            break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run the capture and generation of the dataset, remember that the game's window must be in 1600x900 resolution and located in the top left part of the screen ((0,0) coordinates).\n",
    "\n",
    "Once you captured all the data you need you can press 'J' to stop the run function and archive the current sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plots images from the data file\n",
    "'''\n",
    "\n",
    "plotImgs = True\n",
    "if plotImgs:\n",
    "    with np.load('Data\\\\training_data0.npz') as data:\n",
    "        training_data = data['data']\n",
    "    \n",
    "    i= 10\n",
    "    for j in range(5):\n",
    "        plt.imshow(cv2.cvtColor(training_data[i][j], cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        plt.imshow(cv2.cvtColor(training_data[i][j], cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        print('---------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plots images from the data file after object detection and separation\n",
    "'''\n",
    "\n",
    "plotImgs = True\n",
    "if plotImgs:\n",
    "    with np.load('Data\\\\training_data0.npz') as data:\n",
    "        training_data = data['data']\n",
    "    \n",
    "    i= 10\n",
    "    \n",
    "    train_objects = []\n",
    "    train_background = []\n",
    "    for j in range(5):\n",
    "        temp1,temp2 = get_sub_images(training_data[i][j])\n",
    "        train_objects.append(temp1)\n",
    "        train_background.append(temp2)\n",
    "    \n",
    "    for j in range(5):\n",
    "        plt.imshow(cv2.cvtColor(train_objects[j], cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        plt.imshow(cv2.cvtColor(train_background[j], cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        print('---------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to count the number of instances per class in a set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Rewrite function to use one bit hot encoded values\n",
    "\n",
    "def number_instances_per_class(data):\n",
    "    \n",
    "    nonekey = []\n",
    "    A = []\n",
    "    D = []\n",
    "    W = []\n",
    "    S = []\n",
    "    \n",
    "    AD = []\n",
    "    AW = []\n",
    "    AS = []\n",
    "    DW =[]\n",
    "    DS = []\n",
    "    WS =[]\n",
    "    \n",
    "    ADW = []\n",
    "    AWS =[]\n",
    "    ADS = []\n",
    "    DWS = []\n",
    "    \n",
    "    ASWS = []\n",
    "    \n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    for d in data:\n",
    "        if np.array_equal(d, [0,0,0,0]):\n",
    "            nonekey.append(d)\n",
    "        elif np.array_equal(d, [1,0,0,0]):\n",
    "            A.append(d)\n",
    "        elif np.array_equal(d, [0,1,0,0]):\n",
    "            D.append(d)\n",
    "        elif np.array_equal(d , [0,0,1,0]):\n",
    "            W.append(d)\n",
    "        elif np.array_equal(d , [0,0,0,1]):\n",
    "            S.append(d)\n",
    "        elif np.array_equal(d , [1,1,0,0]):\n",
    "            AD.append(d)\n",
    "        elif np.array_equal(d , [1,0,1,0]):\n",
    "            AW.append(d)\n",
    "        elif np.array_equal(d , [1,0,0,1]):\n",
    "            AS.append(d)\n",
    "        elif np.array_equal(d , [0,1,1,0]):\n",
    "            DW.append(d)\n",
    "        elif np.array_equal(d , [0,1,0,1]):\n",
    "            DS.append(d)\n",
    "        elif np.array_equal(d , [0,0,1,1]):\n",
    "            WS.append(d)\n",
    "        elif np.array_equal(d , [1,1,1,0]):\n",
    "            ADW.append(d)\n",
    "        elif np.array_equal(d , [1,1,0,1]):\n",
    "            AWS.append(d)\n",
    "        elif np.array_equal(d , [1,1,0,1]):\n",
    "            ADS.append(d)\n",
    "        elif np.array_equal(d , [0,1,1,1]):\n",
    "            DWS.append(d)\n",
    "        elif np.array_equal(d , [1,1,1,1]):\n",
    "            ASWS.append(d)\n",
    "    return [nonekey,A,D,W,S,AW,AS,DW,DS]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will balance the number of instances of the classes in a set by deleting extra instances after shuffling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(data_in_clases):\n",
    "    balanced_data = []\n",
    "    data_in_clases.sort(key=len)\n",
    "    max_len = len(data_in_clases[0])\n",
    "        \n",
    "    for data in data_in_clases:\n",
    "        if len(data) > max_len:\n",
    "            data=data[:max_len]\n",
    "        for d in data:\n",
    "            balanced_data.append(d)\n",
    "        \n",
    "    np.random.shuffle(balanced_data)\n",
    "    \n",
    "    return balanced_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For debug purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "if debug:\n",
    "    with np.load('Data\\\\training_data_test.npz') as data:\n",
    "        images = data['data']\n",
    "        labels = data['labels']\n",
    "\n",
    "    number = number_instances_per_class(labels)\n",
    "    print('none: ' + str(len(number[0])))\n",
    "    print('A: ' + str(len(number[1])))\n",
    "    print('D ' + str(len(number[2])))\n",
    "    print('W ' + str(len(number[3])))\n",
    "    print('S ' + str(len(number[4])))\n",
    "    print('AW ' + str(len(number[5])))\n",
    "    print('AS ' + str(len(number[6])))\n",
    "    print('DW ' + str(len(number[7])))\n",
    "    print('DS ' + str(len(number[8])))\n",
    "\n",
    "    #TODO: re-implement data balancing\n",
    "    balanced_data = balance_data(number)\n",
    "    number = number_instances_per_class(balanced_data)\n",
    "    print('none: ' + str(len(number[0])))\n",
    "    print('A: ' + str(len(number[1])))\n",
    "    print('D ' + str(len(number[2])))\n",
    "    print('W ' + str(len(number[3])))\n",
    "    print('S ' + str(len(number[4])))\n",
    "    print('AW ' + str(len(number[5])))\n",
    "    print('AS ' + str(len(number[6])))\n",
    "    print('DW ' + str(len(number[7])))\n",
    "    print('DS ' + str(len(number[8])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the model to use in our training, please select the model you want to use in the cell bellow (the models are described in the \"Definition of the DNN\" of the [documentation](https://github.com/eritzyg/GTAV-Self-driving-car#documentation)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model to use:\n",
    "# 'CNN+MPL' : Convolutional neural network with multi layer perceptron.\n",
    "# 'CNN+RNN' : Convolutional neural network with recurrent neural network.\n",
    "# 'YOLO'    : Processes the objects and the background seperately \n",
    "selected_model = 'YOLO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we define the corresponding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_model == 'CNN+RNN':\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(InputLayer(input_shape=(5, 270, 480, 3)))\n",
    "\n",
    "    model.add(TimeDistributed(Convolution2D(32, (4,4), data_format='channels_last')))\n",
    "    model.add(TimeDistributed(Activation('relu')))\n",
    "    print(model.output_shape)\n",
    "\n",
    "    model.add(TimeDistributed(Convolution2D(32, (4,4), data_format='channels_last')))\n",
    "    model.add(TimeDistributed(Activation('relu')))\n",
    "    print(model.output_shape)\n",
    "\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(5, 5), data_format='channels_last')))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    print(model.output_shape)\n",
    "\n",
    "    model.add(TimeDistributed(Convolution2D(16, (3,3), data_format='channels_last')))\n",
    "    model.add(TimeDistributed(Activation('relu')))\n",
    "    print(model.output_shape)\n",
    "\n",
    "\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(5, 5), data_format='channels_last')))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    print(model.output_shape)\n",
    "\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    print(model.output_shape)\n",
    "\n",
    "    model.add(GRU(256, kernel_initializer=initializers.RandomNormal(stddev=0.001))) #128\n",
    "    model.add(Dropout(0.25))\n",
    "    print(model.output_shape)\n",
    "\n",
    "    model.add(Dense(100))\n",
    "    print(model.output_shape)\n",
    "\n",
    "    model.add(Dense(80))\n",
    "    print(model.output_shape)\n",
    "\n",
    "    model.add(Dense(40))\n",
    "    print(model.output_shape)\n",
    "\n",
    "    model.add(Dense(9, activation='sigmoid'))\n",
    "    print(model.output_shape)\n",
    "\n",
    "    opt = optimizers.RMSprop(lr=0.001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=opt, metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_model == 'CNN+MLP':\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(InputLayer(input_shape=(5, 270, 480, 3)))\n",
    "\n",
    "    model.add(TimeDistributed(Convolution2D(16, (4,8), data_format='channels_last')))\n",
    "    model.add(TimeDistributed(Activation('relu')))\n",
    "    print(model.output_shape)\n",
    "\n",
    "    model.add(TimeDistributed(Convolution2D(16, (4,4), data_format='channels_last')))\n",
    "    model.add(TimeDistributed(Activation('relu')))\n",
    "    print(model.output_shape)\n",
    "\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(5, 5), data_format='channels_last')))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    print(model.output_shape)\n",
    "\n",
    "    model.add(TimeDistributed(Convolution2D(12, (3,3), data_format='channels_last')))\n",
    "    model.add(TimeDistributed(Activation('relu')))\n",
    "    print(model.output_shape)\n",
    "\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(5, 5), data_format='channels_last')))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    print(model.output_shape)\n",
    "\n",
    "    model.add(Flatten())\n",
    "    print(model.output_shape)\n",
    "\n",
    "    model.add(Dense(300))\n",
    "    print(model.output_shape)\n",
    "    model.add(Dense(100))\n",
    "    print(model.output_shape)\n",
    "    print(model.output_shape)\n",
    "    model.add(Dense(9, activation='sigmoid'))\n",
    "    print(model.output_shape)\n",
    "\n",
    "    opt = optimizers.rmsprop(lr=0.001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=opt, metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model by Evan Miller\n",
    "#\n",
    "# Takes in two inputs corresponding to the images after they have been split into objects and background\n",
    "# by the YOLO object detection algorithm. These images are processed with a CNN and an RNN+CNN respectively, then they are\n",
    "# concatenated and fed through some Dense layers for a final prediction.\n",
    "\n",
    "if selected_model == 'YOLO':\n",
    "    input1 = tf.keras.Input(shape=(5, 416, 416, 3))\n",
    "    x1 = TimeDistributed(Convolution2D(16, (4,8), data_format='channels_last'))(input1)\n",
    "    x1 = TimeDistributed(Activation('relu'))(x1)\n",
    "    x1 = TimeDistributed(Convolution2D(16, (4,4), data_format='channels_last'))(x1)\n",
    "    x1 = TimeDistributed(Activation('relu'))(x1)\n",
    "    x1 = TimeDistributed(MaxPooling2D(pool_size=(5, 5), data_format='channels_last'))(x1)\n",
    "    x1 = TimeDistributed(Dropout(0.25))(x1)\n",
    "    x1 = TimeDistributed(Convolution2D(12, (3,3), data_format='channels_last'))(x1)\n",
    "    x1 = TimeDistributed(Activation('relu'))(x1)\n",
    "    x1 = TimeDistributed(MaxPooling2D(pool_size=(5, 5), data_format='channels_last'))(x1)\n",
    "    x1 = TimeDistributed(Dropout(0.25))(x1)\n",
    "    x1 = Flatten()(x1)\n",
    "    x1 = Dense(300)(x1)\n",
    "    x1 = Dense(100)(x1)\n",
    "    #x1 = Dense(9, activation='sigmoid')(x1)\n",
    "    x1 = Model(inputs=input1,outputs=x1)\n",
    "    \n",
    "    input2 = tf.keras.Input(shape=(5, 416, 416, 3))\n",
    "    x2 = TimeDistributed(Convolution2D(32, (4,4), data_format='channels_last'))(input2)\n",
    "    x2 = TimeDistributed(Activation('relu'))(x2)\n",
    "    x2 = TimeDistributed(Convolution2D(32, (4,4), data_format='channels_last'))(x2)\n",
    "    x2 = TimeDistributed(Activation('relu'))(x2)\n",
    "    x2 = TimeDistributed(MaxPooling2D(pool_size=(5, 5), data_format='channels_last'))(x2)\n",
    "    x2 = TimeDistributed(Dropout(0.25))(x2)\n",
    "    x2 = TimeDistributed(Convolution2D(16, (3,3), data_format='channels_last'))(x2)\n",
    "    x2 = TimeDistributed(Activation('relu'))(x2)\n",
    "    x2 = TimeDistributed(MaxPooling2D(pool_size=(5, 5), data_format='channels_last'))(x2)\n",
    "    x2 = TimeDistributed(Dropout(0.25))(x2)\n",
    "    x2 = TimeDistributed(Flatten())(x2)\n",
    "    x2 = GRU(256, kernel_initializer=initializers.RandomNormal(stddev=0.001))(x2)\n",
    "    x2 = Dropout(0.25)(x2)\n",
    "    x2 = Flatten()(x2)\n",
    "    x2 = Dense(100)(x2)\n",
    "    #x2 = Dense(80)(x2)\n",
    "    #x2 = Dense(40)(x2)\n",
    "    #x2 = Dense(9, activation='sigmoid')(x2)\n",
    "    x2 = Model(inputs=input2,outputs=x2)\n",
    "    \n",
    "    combined = concatenate([x1.output,x2.output])\n",
    "    out = Dense(80)(combined)\n",
    "    out = Dense(40)(out)\n",
    "    out = Dense(9, activation='sigmoid')(out)\n",
    "    \n",
    "    model = Model(inputs=[x1.input,x2.input], outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 5, 416, 416, 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, 5, 413, 413,  1568        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 5, 416, 416, 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistri (None, 5, 413, 413,  0           time_distributed_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 5, 413, 409,  1552        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_12 (TimeDistri (None, 5, 410, 410,  16416       time_distributed_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 5, 413, 409,  0           time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_13 (TimeDistri (None, 5, 410, 410,  0           time_distributed_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 5, 410, 406,  4112        time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_14 (TimeDistri (None, 5, 82, 82, 32 0           time_distributed_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 5, 410, 406,  0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_15 (TimeDistri (None, 5, 82, 82, 32 0           time_distributed_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 5, 82, 81, 16 0           time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_16 (TimeDistri (None, 5, 80, 80, 16 4624        time_distributed_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 5, 82, 81, 16 0           time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_17 (TimeDistri (None, 5, 80, 80, 16 0           time_distributed_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 5, 80, 79, 12 1740        time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_18 (TimeDistri (None, 5, 16, 16, 16 0           time_distributed_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 5, 80, 79, 12 0           time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_19 (TimeDistri (None, 5, 16, 16, 16 0           time_distributed_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, 5, 16, 15, 12 0           time_distributed_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_20 (TimeDistri (None, 5, 4096)      0           time_distributed_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, 5, 16, 15, 12 0           time_distributed_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       (None, 256)          3343872     time_distributed_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 14400)        0           time_distributed_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 256)          0           gru[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 300)          4320300     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 256)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          30100       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          25700       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 200)          0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 80)           16080       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 40)           3240        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 9)            369         dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,769,673\n",
      "Trainable params: 7,769,673\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define some functions to reshape the data according to the input of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine value range for images from the game. \n",
    "\n",
    "def reshape_custom_X(data, verbose = 1):\n",
    "    reshaped = np.zeros((data.shape[0], 5, 416, 416, 3), dtype=np.float32)\n",
    "    for i in range(0, data.shape[0]):\n",
    "        for j in range(0, 5):\n",
    "            if (verbose == 1):\n",
    "                clear_output(wait=True)\n",
    "                stdout.write('Reshaped image: ' + str(i))\n",
    "                stdout.flush()\n",
    "            reshaped[i][j] = data[i][j]/255.\n",
    "            \n",
    "    return reshaped\n",
    "\n",
    "def reshape_custom_y(data):\n",
    "    reshaped = np.zeros((data.shape[0], 9), dtype=np.float32)\n",
    "    for i in range(0, data.shape[0]):\n",
    "        if np.array_equal(data[i] , [0,0,0,0]):\n",
    "            reshaped[i][0] = 1.\n",
    "        elif np.array_equal(data[i] , [1,0,0,0]):\n",
    "            reshaped[i][1] = 1.\n",
    "        elif np.array_equal(data[i] , [0,1,0,0]):\n",
    "            reshaped[i][2] = 1.\n",
    "        elif np.array_equal(data[i] , [0,0,1,0]):\n",
    "            reshaped[i][3] = 1.\n",
    "        elif np.array_equal(data[i] , [0,0,0,1]):\n",
    "            reshaped[i][4] = 1.\n",
    "        elif np.array_equal(data[i] , [1,0,1,0]):\n",
    "            reshaped[i][5] = 1.\n",
    "        elif np.array_equal(data[i] , [1,0,0,1]):\n",
    "            reshaped[i][6] = 1.\n",
    "        elif np.array_equal(data[i] , [0,1,1,0]):\n",
    "            reshaped[i][7] = 1.\n",
    "        elif np.array_equal(data[i] , [0,1,0,1]):\n",
    "            reshaped[i][8] = 1.\n",
    "    return reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions will help for picking the number of batches and the start and end indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_batches(length, BATCH_SIZE):\n",
    "    if (int(length/BATCH_SIZE)*BATCH_SIZE == length):\n",
    "        return int(length/BATCH_SIZE)\n",
    "    else:\n",
    "        return int(length/BATCH_SIZE)+1\n",
    "\n",
    "def get_start_end(iteration, BATCH_SIZE, max_length):\n",
    "    start = iteration*BATCH_SIZE\n",
    "    if (start > max_length):\n",
    "        print(\"ERROR: Check iterations made! Must be wrong\")\n",
    "        return -1, -1\n",
    "    end = (iteration+1)*BATCH_SIZE\n",
    "    if (end > max_length):\n",
    "        end = max_length\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we run the training process, the accuracy can be seen after each epoch (all the inputs will be saved to a \"log.txt\" file, so don't worry if you miss out some output). Also remember to change the path to the same path where you saved the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped image: 2\n",
      " Training..."
     ]
    }
   ],
   "source": [
    "log.openlog()\n",
    "\n",
    "# Define the sizes and epoches\n",
    "BATCH_SIZE = 10\n",
    "TEST_BATCH_SIZE = 10\n",
    "n_epochs = 6\n",
    "\n",
    "# This path will be the path used to load the dataset files!\n",
    "files = glob.glob(\"Data\\\\*.npz\")\n",
    "\n",
    "\n",
    "actual_file = 0\n",
    "acc_for_files = []\n",
    "for fil in files:\n",
    "    actual_file = actual_file + 1\n",
    "    log.output(\"\\n Loading input: \"+fil)\n",
    "    with np.load(fil) as data:\n",
    "        training_data_images = data['data']\n",
    "        training_data_labels = data['labels']\n",
    "\n",
    "    #print(\"\\n Balancing data...\")\n",
    "    #number = number_instances_per_class(training_data)\n",
    "    #training_data = np.array(balance_data(number))\n",
    "    \n",
    "    log.output(\"\\n Reshaping data...\")\n",
    "    \n",
    "    to_shuffle = list(zip(training_data_images, training_data_labels))\n",
    "    np.random.shuffle(to_shuffle)\n",
    "    training_data_images, training_data_labels = zip(*to_shuffle)\n",
    "    \n",
    "    #np.random.shuffle(training_data)\n",
    "    train_x = training_data_images[0:int(len(training_data_images)*0.90)]\n",
    "    train_y = training_data_labels[0:int(len(training_data_labels)*0.90)]\n",
    "    test_x = training_data_images[int(len(training_data_images)*0.90):]\n",
    "    test_y = training_data_labels[int(len(training_data_labels)*0.90):]\n",
    "    #test = training_data[int(len(training_data)*0.90):len(training_data)]\n",
    "    del training_data_images, training_data_labels\n",
    "    \n",
    "    log.output(\"\\n Executing Object Detection...\")\n",
    "    \n",
    "    train_objects = []\n",
    "    train_background = []\n",
    "    for i in range(len(train_x)):\n",
    "        train_objects.append([])\n",
    "        train_background.append([])\n",
    "        for j in range(5):\n",
    "            temp1,temp2 = get_sub_images(train_x[i][j])\n",
    "            train_objects[i].append(temp1)\n",
    "            train_background[i].append(temp2)\n",
    "    \n",
    "    test_objects = []\n",
    "    test_background = []\n",
    "    for i in range(len(test_x)):\n",
    "        test_objects.append([])\n",
    "        test_background.append([])\n",
    "        for j in range(5):\n",
    "            temp1,temp2 = get_sub_images(test_x[i][j])\n",
    "            test_objects[i].append(temp1)\n",
    "            test_background[i].append(temp2)\n",
    "    \n",
    "    X_train = [reshape_custom_X(np.array(train_objects)), reshape_custom_X(np.array(train_background))]\n",
    "    y_train = reshape_custom_y(np.array(train_y))\n",
    "    X_test = [reshape_custom_X(np.array(test_objects)), reshape_custom_X(np.array(test_background))]\n",
    "    y_test= reshape_custom_y(np.array(test_y))\n",
    "    del train_x, train_y, test_x, test_y, test_objects, test_background, train_objects, train_background\n",
    "    \n",
    "    log.output(\"\\n Training...\")\n",
    "    data_length = len(X_train[0])\n",
    "    n_batch = get_num_batches(data_length, BATCH_SIZE)\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(n_batch):\n",
    "            start, end = get_start_end(iteration, BATCH_SIZE, data_length)\n",
    "            model.fit(x=[X_train[0][start:end], X_train[1][start:end]], y=y_train[start:end], epochs=1, verbose=0)\n",
    "            clear_output(wait=True)\n",
    "            log.output('\\n => File : ' + str(actual_file) + ' of ' + str(len(files)))\n",
    "            log.output('\\n ==> EPOCH : ' + str(epoch+1) + ' of ' + str(n_epochs))\n",
    "            log.output('\\n ===> Iteration: ' + str(iteration+1) + ' of ' + str(n_batch))\n",
    "            score = model.evaluate((X_train[0][start:end], X_train[1][start:end]), y_train[start:end], verbose=0)\n",
    "            log.output(\"\\n Train batch accuracy: %.2f%%\" % (score[1]*100))\n",
    "            stdout.flush()\n",
    "        prec = np.zeros((9))\n",
    "        for i in range(len(y_test)):\n",
    "            p = np.argmax(model.predict((X_test[0][i:i+1], X_test[1][i:i+1])), axis=1)\n",
    "            prec[p] = prec[p] + 1\n",
    "        log.output(\"\\n ==> Predictions:\"+str(prec))\n",
    "        stdout.flush()   \n",
    "    total_acc = 0.0\n",
    "    num = 0\n",
    "    data_length = len(X_test[0])\n",
    "    n_batch = get_num_batches(data_length, TEST_BATCH_SIZE)\n",
    "    for iteration in range(n_batch):\n",
    "        start, end = get_start_end(iteration, TEST_BATCH_SIZE, data_length)\n",
    "        score = model.evaluate((X_test[0][start:end], X_test[1][start:end]), y_test[start:end], verbose=0)\n",
    "        log.output(\"\\n => Batch %s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "        total_acc = total_acc + score[1]\n",
    "        num = num + 1\n",
    "    total_acc = total_acc/float(num)\n",
    "    log.output(\"\\n ==> Total acc for file %s: %.2f%%\" % (fil, total_acc*100))\n",
    "    acc_for_files.append(total_acc*100)\n",
    "\n",
    "\n",
    "    prec = np.zeros((9))\n",
    "    for i in range(len(y_test)):\n",
    "        p = np.argmax(model.predict((X_test[0][i:i+1], X_test[1][i:i+1])), axis=1)\n",
    "        prec[p] = prec[p] + 1\n",
    "    log.output(\"\\n ==> Predictions:\"+str(prec))\n",
    "    time.sleep(5)\n",
    "\n",
    "for acc in range(len(acc_for_files)):\n",
    "    log.output(\"\\n ==> Total acc after file %d: %.2f%%\" % (acc, acc_for_files[acc]))\n",
    "    \n",
    "    \n",
    "log.closelog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save the learned network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py as h5py\n",
    "#model.save('Model\\\\'+selected_model)\n",
    "model.save('Model\\\\'+'insider')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plots images from the data file after object detection and separation, then executes and prints a prediction on the sequence\n",
    "'''\n",
    "\n",
    "#model = load_model('Data\\\\'+selected_model)\n",
    "model = load_model('Model\\\\'+'insider')\n",
    "\n",
    "plotImgs = True\n",
    "if plotImgs:\n",
    "    with np.load('Data\\\\training_data_test.npz') as data:\n",
    "        training_data = data['data']\n",
    "    \n",
    "    i= 10\n",
    "    \n",
    "    train_objects = []\n",
    "    train_background = []\n",
    "    for j in range(5):\n",
    "        temp1,temp2 = get_sub_images(training_data[i][j])\n",
    "        train_objects.append(temp1)\n",
    "        train_background.append(temp2)\n",
    "    \n",
    "    for j in range(5):\n",
    "        plt.imshow(cv2.cvtColor(train_objects[j], cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        plt.imshow(cv2.cvtColor(train_background[j], cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        print('---------------------------------------------------------')\n",
    "    \n",
    "    X_test = [reshape_custom_X(np.array([train_objects]), verbose=0), reshape_custom_X(np.array([train_background]), verbose=0)]\n",
    "\n",
    "    print (np.argmax(model.predict((X_test[0], X_test[1])), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run our model in the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to test our model in the game, we will reduce the amount of VRAM tensorflow can use so that the game has some VRAM in spare. We will also import the library needed to send inputs to the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let the game have some VRAM (needed or the game will crash)\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n",
    "\n",
    "# For controlling the game\n",
    "from inputsHandler import select_key\n",
    "from tkinter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_press(key):\n",
    "    if key == 1:\n",
    "        return'A'\n",
    "    if key == 2:\n",
    "        return'D'\n",
    "    if key == 3:\n",
    "        return'W'\n",
    "    if key == 4:\n",
    "        return'S'\n",
    "    if key == 5:\n",
    "        return'AW'\n",
    "    if key == 6:\n",
    "        return'AS'\n",
    "    if key == 7:\n",
    "        return'DW'\n",
    "    if key == 8:\n",
    "        return'DS'\n",
    "    return 'none'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fuction calculates the Mean Squared Error between 2 images. Is used to detect if the car is stuck somewhere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(imageA, imageB):\n",
    "\n",
    "    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "    err /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the run function for our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Configurations\n",
    "show_current_control = False #It will show a windows with a message indicating if the car is currently be controlled by\n",
    "                            #Network  or by a Human\n",
    "    \n",
    "show_whatAIsees = False #It will show the 5 images that the netowrk uses the predict the output \n",
    "\n",
    "enable_evasion = False #If the program detects that the car is not moving (for example because it is stuck facing a wall and\n",
    "                        #the network is not able to return to the road) It will make the car move backwards for a second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_IA():\n",
    "    global fps\n",
    "    global front_buffer\n",
    "    global back_buffer\n",
    "    global seq\n",
    "    global key_out\n",
    "    global num\n",
    "    \n",
    "    model = load_model('Data\\\\'+selected_model)\n",
    "    \n",
    "    training_data = []\n",
    "    threads = list()\n",
    "    th_img = threading.Thread(target=img_thread)\n",
    "    th_seq = threading.Thread(target=image_sequencer_thread)\n",
    "    threads.append(th_img)\n",
    "    threads.append(th_seq)\n",
    "    th_img.start()\n",
    "    time.sleep(1)\n",
    "    th_seq.start()\n",
    "    time.sleep(4)\n",
    "    \n",
    "    last_num = 0\n",
    "    \n",
    "    last_time = time.time()\n",
    "    \n",
    "    if show_current_control:\n",
    "        root = Tk()\n",
    "        var = StringVar()\n",
    "        var.set('IA CONDUCIENDO')\n",
    "        l = Label(root, textvariable = var, fg='green', font=(\"Courier\", 44))\n",
    "        l.pack()\n",
    "\n",
    "    \n",
    "    while True:\n",
    "       \n",
    "        img_seq = seq.copy()\n",
    "        while len(img_seq) != 5 or last_num==num:\n",
    "            del img_seq\n",
    "            img_seq = seq.copy()\n",
    "        last_num = num\n",
    "        array = [img_seq[0],img_seq[1],img_seq[2],img_seq[3],img_seq[4]]\n",
    "        \n",
    "        objects = []\n",
    "        background = []\n",
    "        for j in range(5):\n",
    "            temp1,temp2 = get_sub_images(array[j])\n",
    "            objects.append(temp1)\n",
    "            background.append(temp2)\n",
    "        \n",
    "        \n",
    "        objects = np.expand_dims(np.array(objects), axis=0)\n",
    "        background = np.expand_dims(np.array(background), axis=0)\n",
    "        p = np.argmax(model.predict([objects,background]))\n",
    "        \n",
    "        if not 'J' in key_check():\n",
    "            select_key(p)\n",
    "            if show_current_control:\n",
    "                var.set('IA CONDUCIENDO')\n",
    "                l.config(fg='green')\n",
    "                root.update()\n",
    "        else:\n",
    "            if show_current_control:\n",
    "                var.set('CONTROL MANUAL')\n",
    "                l.config(fg='red')\n",
    "                root.update()\n",
    "\n",
    "        #This is used to detect if the car is stuck somewhere (for example facing a wall) and the network does not know what to do. It will move the car\n",
    "        #backward for a second.\n",
    "        \n",
    "        if enable_evasion:\n",
    "            score = mse(img_seq[0],img_seq[4])\n",
    "            if score < 1000:\n",
    "                if show_current_control:\n",
    "                    var.set('MANIOBRA DE EVASIÓN')\n",
    "                    l.config(fg='blue')\n",
    "                    root.update()\n",
    "                select_key(4)\n",
    "                time.sleep(1)\n",
    "                if np.random.rand()>0.5:\n",
    "                    select_key(6)\n",
    "                else:\n",
    "                    select_key(8)\n",
    "                time.sleep(0.2)\n",
    "                if show_current_control:\n",
    "                    var.set('IA CONDUCIENDO')\n",
    "                    l.config(fg='green')\n",
    "                    root.update()\n",
    "\n",
    "        time_act = time.time()\n",
    "        clear_output(wait=True)\n",
    "        stdout.write('Recording at {} FPS \\n'.format(fps))\n",
    "        stdout.write('Images in sequence {} \\n'.format(len(img_seq)))\n",
    "        stdout.write('Keys pressed: ' + key_press(p[0]) + '\\n')\n",
    "        stdout.write('Actions per second: ' + str(1/(time_act-last_time)) + '\\n')\n",
    "        if enable_evasion:\n",
    "            stdout.write('Diference from img 1 to img 5: ' + str(score))\n",
    "        stdout.flush()\n",
    "        last_time = time.time()\n",
    "        \n",
    "        if show_whatAIsees:\n",
    "            cv2.imshow('window1',np.array(img_seq[0])) \n",
    "            cv2.imshow('window2',np.array(img_seq[1]))\n",
    "            cv2.imshow('window3',np.array(img_seq[2]))\n",
    "            cv2.imshow('window4',np.array(img_seq[3]))\n",
    "            cv2.imshow('window5',np.array(img_seq[4]))\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we run the network in the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_IA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
